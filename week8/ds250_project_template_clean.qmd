---
title: "Client Report - Project 4: Can you predict that?"
subtitle: "Course DS 250"
author: "Rebecca Roeth"
format:
  html:
    self-contained: true
    page-layout: full
    title-block-banner: true
    toc: true
    toc-depth: 3
    toc-location: body
    number-sections: false
    html-math-method: katex
    code-fold: true
    code-summary: "Show the code"
    code-overflow: wrap
    code-copy: hover
    code-tools:
        source: false
        toggle: true
        caption: See code
execute: 
  warning: false
    
---

```{python}
#| label: libraries
#| include: false
import pandas as pd
import plotly.express as px
```


## Elevator Pitch

__I have discovered that, when classifying if houses were made before or after 1980, the best classification is how many stories the house has. Additionally, if they have attached garages is another feature to classify homes.__

```{python}
#| label: project-data
#| code-summary: Read and format project data

# Learn morea about Code Cells: https://quarto.org/docs/reference/cells/cells-jupyter.html

# Include and execute your code here
url = "https://github.com/byuidatascience/data4dwellings/raw/master/data-raw/dwellings_ml/dwellings_ml.csv"
data = pd.read_csv(url)
```

__Highlight the Questions and Tasks__

## Valuable Variables

__Create 2-3 charts that evaluate potential relationships between the home variables and before1980. Explain what you learn from the charts that could help a machine learning algorithm.__

I created every graph in an attempt to try to better understand the dataset. I made histograms of every feature and flagged the ones that had different highs and lows for the _before1980s_ feature. For example, when stories had a high count for the low values (one story tall) in before1980=1 and the stories had a low count for the low values (one story tall) in before1980=0, I saved it.

I discovered that the most defining characteristic was how many stories there were.

```{python}
characteristics= ['stories', 'quality_C', 'numbdrm', 'netprice', 'numbaths', 'gartype_Att', 'arcstyle_ONE-STORY']
for i in characteristics:
    graph = px.histogram(data,
        x = i,
        color = "before1980",
        facet_col = "before1980",
        title = f"Histogram of {i} Total"
        )
    graph.show()

```

## Algorithm to Label Houses

__Build a classification model labeling houses as being built “before 1980” or “during or after 1980”. Your goal is to reach or exceed 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried.__

After much evaluation, I got a ~85% accuracy by using the BernoulliNB classifier and three features. Adding more features either made the algorithm worse or exactly the same.

```{python}
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import accuracy_score

features = data.filter([
    'arcstyle_ONE-STORY', 'quality_C', 'gartype_Att'
    ])
targets = data.filter(['before1980'])

train_data, test_data, train_targets, test_targets = train_test_split(features, targets, test_size=0.3)

classifier = BernoulliNB()
classifier.fit(train_data, train_targets)

targets_predicted = classifier.predict(test_data)
print(classifier.score(test_data, test_targets))
```

## Stories

__Justify your classification model by discussing the most important features selected by your model. This discussion should include a chart and a description of the features.__

I knew the stories feature was important. I knew that post WW2 suburban homes were made en-mass to supply the soldiers who came home with benefits that included a generous loan for housing. These houses were typically only one story tall since they were all made to look alike.

I initally used the __stories__ feature but found that using __arcstyle_ONE-STORY__ was more effective because it was counting what was one story and what wasn't.
```{python}
story_test = [data.filter(['stories']), data.filter(['arcstyle_ONE-STORY'])]
targets1 = data.filter(['before1980'])
classifier1 = BernoulliNB()
for i in story_test:
  train_data1, test_data1, train_targets1, test_targets1 =  train_test_split (i, targets1, test_size=0.3)
  classifier1.fit(train_data1, train_targets1) 
  print(f"{i}: ", classifier1.score(test_data1, test_targets1))
```

## Quality
__Describe the quality of your classification model using 2-3 different evaluation metrics. You also need to explain how to interpret each of the evaluation metrics you use.__

I used a confusion matrix to outline the ratios of true and false classifications.
```{python}
from sklearn import metrics
import matplotlib.pyplot as plt

confusion_matrix = metrics.confusion_matrix(test_targets, targets_predicted)
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])
cm_display.plot()
plt.show()
```

The true label for true, 1, or that the house was made before 1980, there is a 3,730 true positives and 583 false negatives. This was the highest number of false classifications which is still only 15% (which follows the ~84% accuracy from earlier).

However, this is the worst cetegory to be wrong in, since the risk is that there could be asbestos in the home. However, it isn't at a higher rate than false positives and is low.

